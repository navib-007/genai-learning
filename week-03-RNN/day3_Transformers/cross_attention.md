### ğŸ” Quick Recap Before Cross-Attention
- In a Transformer:
    - Self-Attention â†’ Token attends to all other tokens in the same sequence.
    - Masked Self-Attention â†’ Token attends only to previous tokens in the same sequence.
    - Cross-Attention â†’ Token attends to another sequence, typically decoder token attends to encoder outputs.

### ğŸ¯ What is Cross-Attention?
- Cross-Attention allows the decoder to attend to the encoder outputs â€” meaning the decoder can â€œlook backâ€ at the encoded input while generating output tokens.

Input to Decoder
      â†“
Masked Self-Attention
      â†“
ğŸ’¡ Cross-Attention â†’ attends to Encoder Output
      â†“
Feed Forward + LayerNorm
![alt text](image-13.png)

### ğŸ“¦ Why Cross-Attention is Powerful
| Feature                                      | Benefit                              |
| -------------------------------------------- | ------------------------------------ |
| Connects encoder & decoder                   | Allows decoder to condition on input |
| Handles variable-length input                | Flexible sequence alignment          |
| Learns alignment (like attention in Seq2Seq) | No manual alignment needed           |

### ğŸ” Difference from Self-Attention
| Property         | Self-Attention            | Cross-Attention                  |
| ---------------- | ------------------------- | -------------------------------- |
| Operates On      | Same sequence             | Between two sequences            |
| Used In          | Encoder & Decoder         | Decoder only                     |
| Inputs (Q, K, V) | Q = K = V (same sequence) | Q from decoder, K/V from encoder |

**Cross-Attention enables the decoder to learn how each output token relates to the input tokens â€” by attending to encoder outputs using the decoderâ€™s queries.**

*Itâ€™s a key bridge between input and output sequences in sequence-to-sequence models.*