### ğŸ—ï¸ What is the Transformer Decoder?
The Decoder is the second half of the original Transformer architecture (after the Encoder).
It generates output one token at a time, using both:
- Previous output tokens (via Masked Self-Attention), and
- Encoded input (via Cross-Attention).

### ğŸ” When is the Decoder Used?
- Translation (e.g., English â†’ French)
- Summarization
- Text Generation (GPT-like models only use the decoder)
- Image Captioning (via encoder-decoder)

### ğŸ§± Decoder Architecture Overview
- Each decoder block (repeated N times, usually 6 or 12) has 3 main sub-layers:
Input Embedding + Positional Encoding
          â†“
1ï¸âƒ£ Masked Multi-Head Self-Attention
          â†“
2ï¸âƒ£ Cross-Attention (queries from decoder, keys/values from encoder)
          â†“
3ï¸âƒ£ Feed Forward Neural Network (FFN)
          â†“
Add & LayerNorm after each block

### ğŸ”¢ 0. Input Embedding + Positional Encoding
Each token in the output sequence is first turned into an embedding:
`ğ‘¥ğ‘¡ = Embedding(ğ‘¤ğ‘¡) + PositionalEncoding(ğ‘¡)`
This gives the model token meaning + position.

### ğŸ” 1. Masked Multi-Head Self-Attention (Causal Attention)
Ensures that each token canâ€™t see the future tokens â€” essential for generation.

Step-by-step:
For each token, compute:
![alt text](image-14.png)
- X = input embeddings (previously generated tokens)
- WQ, WK, WV : learnable weights
- M: mask matrix â€” upper triangular with âˆ’âˆ to block future tokens
- ğ‘‘ğ‘˜: dimensionality of key/query
- Softmax distributes weights over past and current tokens only

### ğŸ” 2. Cross-Attention (Decoder â†’ Encoder Output)
Enables the decoder to attend to encoder outputs while generating.
![alt text](image-15.png)

### ğŸ§® 3. Feed Forward Neural Network (FFN)
Adds capacity to model complex relationships.

Each token vector is passed independently through:
![alt text](image-16.png)

### ğŸ§¼ Layer Normalization + Residual Connection
After each sub-layer, the output is:
`Output=LayerNorm(x+SubLayer(x))`
- This stabilizes learning and allows deeper models by helping gradients flow.

### ğŸ”š Final Linear Layer + Softmax
At the end of the last decoder block:
`Logits = DecoderOutput â‹… ğ‘Šğ‘‡`
`Probabilities = Softmax(Logits)`

Where:
- ğ‘Šğ‘‡ : weight matrix shared with embedding layer (tied embeddings)
- Softmax gives probabilities over the vocabulary.

### âœ… Summary Table
| Component             | Role                                             |
| --------------------- | ------------------------------------------------ |
| Masked Self-Attention | Attend to past tokens only (causal generation)   |
| Cross-Attention       | Attend to encoder outputs (align input & output) |
| Feed-Forward Network  | Increase representation power                    |
| LayerNorm + Residual  | Stabilize gradients and training                 |
| Final Softmax Layer   | Convert to probability over vocabulary           |
