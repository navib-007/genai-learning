# Deep Learning
## Deep Learning is a subfield of Machine Learning. It uses neural networks with many layers (hence deep) to model complex patterns in large datasets.

# Reason | Why It Matters
##  📈 Big Data | Old ML couldn't handle huge unstructured data like images, audio, video. DL thrives on it.
## 🚀 Computing Power | GPUs made it possible to train large neural networks fast.
## 🔍 Feature Engineering | Traditional ML needs manual feature selection. DL learns features automatically.
## 🔮 Better Accuracy | DL models outperform ML in many domains like image recognition, speech, NLP.

# Deep learning came because traditional ML struggled with complex data and scalability.

# Deep learning is part of broader family of ML methods based on ANN with representational learning(Feature Engineering).

# DL uses multiple layers to progressively extract higher level features from the raw input.
## For ex: In image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to human such as letters, digits or faces

## Machine Learning | Deep Learning
### Often requires manual feature extraction (you select important variables) | Learns features automatically from raw data (e.g., pixels, text)
### Works well on small to medium datasets |	Needs large datasets to perform well
### Interpretable | Not easy to interpret results as it is blackbox
### Runs on CPU; less hardware-intensive	| Requires GPUs/TPUs for training deep networks
### Linear Regression, Decision Tree, SVM, Random Forest	| CNN (for images), RNN (for sequences), Transformer (for text), Autoencoders
### Fraud detection, forecasting, recommendation systems	| Face recognition, speech-to-text, chatbots, self-driving cars


## Types of NN
### 1. Feedforward Neural Network (FNN) – Basic & Most Common - Used in: Basic classification/regression | Info flows in one direction: Input → Hidden → Output | No cycles or loops. | Used in: Basic classification/regression | Think of it like a layered pipeline — each layer processes and passes forward.

### 2. Convolutional Neural Network (CNN) – Great for Images Uses convolutional layers to detect features (edges, shapes, etc.) | Often followed by pooling and fully connected layers. | Used in: Image classification, object detection, medical imaging Like how our eyes recognize faces, letters, etc.

### 3. Recurrent Neural Network (RNN) – Good for Sequences | Has loops (memory of previous input) | Can process sequential data. | Used in: Time-series forecasting, speech recognition, language modeling | Think of it like remembering words in a sentence as you read. -->

### 4. Long Short-Term Memory (LSTM) – Advanced RNN | A special kind of RNN that remembers long-term dependencies.| Avoids vanishing gradient problems. | Used in: Text generation, translation, music generation | It’s like an RNN with a better memory.

### 5. Generative Adversarial Network (GAN) – For Creating Data | Has two networks: Generator: tries to create fake data. Discriminator: tries to detect fake vs real. They compete and improve each other. | Used in: Deepfakes, image synthesis, art generation

### 6. Transformer Networks – Best for Text and NLP | Uses self-attention to weigh the importance of words. No loops, handles long sequences efficiently. | Used in: ChatGPT, BERT, translation, summarization | It's the architecture behind most modern AI language models.

## Application of DL:
### Self Driving Cars
### Game Playing agents
### Virtual Assistant (Chatbots)
### Image Colourization
### Adding audio to mute video
### Image caption Generation
### Text Translation
### Pixel Restoration
### Object Detection
### GAN
### Deep Dreaming

