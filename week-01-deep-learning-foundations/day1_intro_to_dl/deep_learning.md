# Deep Learning
## Deep Learning is a subfield of Machine Learning. It uses neural networks with many layers (hence deep) to model complex patterns in large datasets.

# Reason | Why It Matters
##  ðŸ“ˆ Big Data | Old ML couldn't handle huge unstructured data like images, audio, video. DL thrives on it.
## ðŸš€ Computing Power | GPUs made it possible to train large neural networks fast.
## ðŸ” Feature Engineering | Traditional ML needs manual feature selection. DL learns features automatically.
## ðŸ”® Better Accuracy | DL models outperform ML in many domains like image recognition, speech, NLP.

# Deep learning came because traditional ML struggled with complex data and scalability.

# Deep learning is part of broader family of ML methods based on ANN with representational learning(Feature Engineering).

# DL uses multiple layers to progressively extract higher level features from the raw input.
## For ex: In image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to human such as letters, digits or faces

## Machine Learning | Deep Learning
### Often requires manual feature extraction (you select important variables) | Learns features automatically from raw data (e.g., pixels, text)
### Works well on small to medium datasets |	Needs large datasets to perform well
### Interpretable | Not easy to interpret results as it is blackbox
### Runs on CPU; less hardware-intensive	| Requires GPUs/TPUs for training deep networks
### Linear Regression, Decision Tree, SVM, Random Forest	| CNN (for images), RNN (for sequences), Transformer (for text), Autoencoders
### Fraud detection, forecasting, recommendation systems	| Face recognition, speech-to-text, chatbots, self-driving cars


## Types of NN
### 1. Feedforward Neural Network (FNN) â€“ Basic & Most Common - Used in: Basic classification/regression | Info flows in one direction: Input â†’ Hidden â†’ Output | No cycles or loops. | Used in: Basic classification/regression | Think of it like a layered pipeline â€” each layer processes and passes forward.

### 2. Convolutional Neural Network (CNN) â€“ Great for Images Uses convolutional layers to detect features (edges, shapes, etc.) | Often followed by pooling and fully connected layers. | Used in: Image classification, object detection, medical imaging Like how our eyes recognize faces, letters, etc.

### 3. Recurrent Neural Network (RNN) â€“ Good for Sequences | Has loops (memory of previous input) | Can process sequential data. | Used in: Time-series forecasting, speech recognition, language modeling | Think of it like remembering words in a sentence as you read. -->

### 4. Long Short-Term Memory (LSTM) â€“ Advanced RNN | A special kind of RNN that remembers long-term dependencies.| Avoids vanishing gradient problems. | Used in: Text generation, translation, music generation | Itâ€™s like an RNN with a better memory.

### 5. Generative Adversarial Network (GAN) â€“ For Creating Data | Has two networks: Generator: tries to create fake data. Discriminator: tries to detect fake vs real. They compete and improve each other. | Used in: Deepfakes, image synthesis, art generation

### 6. Transformer Networks â€“ Best for Text and NLP | Uses self-attention to weigh the importance of words. No loops, handles long sequences efficiently. | Used in: ChatGPT, BERT, translation, summarization | It's the architecture behind most modern AI language models.

## Application of DL:
### Self Driving Cars
### Game Playing agents
### Virtual Assistant (Chatbots)
### Image Colourization
### Adding audio to mute video
### Image caption Generation
### Text Translation
### Pixel Restoration
### Object Detection
### GAN
### Deep Dreaming

