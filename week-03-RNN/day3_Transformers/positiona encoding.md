![alt text](image-6.png)

## Positional Encoding:
### â“ Why Do We Need Positional Encoding?
* Unlike RNNs or CNNs, Transformers donâ€™t process tokens in order â€” they see the input as a set, not a sequence.
**ğŸ§  But language has structure â€” "The dog bit the man" â‰  "The man bit the dog".**
* So we need to inject information about position into each token's embedding.
* This is done via Positional Encoding.

### ğŸ§® What is Positional Encoding?
* Itâ€™s a vector added to the word embedding to indicate the position of the word in the sequence.
* Letâ€™s say:
- - Input: "The cat sat"
- - Position indices: 0 â†’ "The", 1 â†’ "cat", 2 â†’ "sat"
- - Each token embedding is combined with a position vector that encodes where it appears in the sequence.

### ğŸ“˜ Formula: Sinusoidal Positional Encoding (Used in Transformers)
* For position pos and dimension i in the embedding:
![alt text](image-7.png)

### âœ… Properties:
* Uses sinusoidal waves of different frequencies
* Allows the model to learn relative positions
* Does not require learning parameters

### ğŸ”„ How It Works
* For each word, we do:
**FinalÂ Input=WordEmbedding + PositionalEncoding**
* This gives each token not only what it is (via embedding) but also where it is.

| Feature               | Advantage                                 |
| --------------------- | ----------------------------------------- |
| No learned parameters | Efficient and simple                      |
| Can generalize        | Works for longer sequences at inference   |
| Encodes distances     | Enables model to infer relative positions |

**Positional Encoding gives each word a sense of where it appears in the sentence â€” enabling Transformers to understand order, even though they process words in parallel.**