## Transformer (Encoder)

* The encoder takes an input sequence (e.g., a sentence) and turns it into a `context-aware vector representation` â€” meaning it understands each word in the context of the whole sentence.

* It does this through stacked layers, each containing:
- - Multi-Head Self-Attention
- - Feed Forward Network
- - Layer Normalization
- - Residual Connections

![alt text](image-9.png)

Input Embedding
     â†“
Positional Encoding
     â†“
[1] Multi-Head Self-Attention
     â†“
[2] Add & LayerNorm
     â†“
[3] Feed Forward Network
     â†“
[4] Add & LayerNorm
     â†“
Output to next encoder layer or decoder

### ğŸ”¹ 1. Input Embedding + Positional Encoding
- ğŸ”  Word Embedding:
* Each word/token is converted to a vector (e.g., 512-dimensional) using an embedding layer.

- ğŸ§­ Positional Encoding:
* Since Transformers don't have recurrence, we add position info using a sinusoidal function:
**Inputğ‘– = Embeddingğ‘– + PositionalEncodingğ‘–**
* So the model knows: â€œcatâ€ came after â€œTheâ€, not before.

### ğŸ”¹ 2. Multi-Head Self-Attention (MHSA)
- ğŸ§  Goal:
* Let each token pay attention to all other tokens in the sequence and weigh them differently.
* For each token:
Create Query (Q): What am I looking for?
Create Key (K): What do I offer?
Create Value (V): What information do I hold?
![alt text](image-10.png)
* This gives a weighted sum of all tokens â†’ capturing their context.

### ğŸ”¹ 3. Add & LayerNorm (Post-Attention)
- ğŸ§® Residual Connection:
* We add the input to the attention output:
- Out1 = LayerNorm(Input + MHSA)
- This helps gradients flow better and prevents vanishing/exploding.

- ğŸ§ª LayerNorm:
* Normalizes across the features to stabilize learning.

### ğŸ”¹ 4. Feed-Forward Neural Network (FFN)
- Every tokenâ€™s output is passed independently through a 2-layer MLP:
![alt text](image-11.png)
- This helps the model:
* Transform and project information
* Increase model capacity

### ğŸ”¹ 5. Add & LayerNorm (Post-FFN)
- Again, we apply residual connection + normalization:
- - Out2 = LayerNorm(Out1 + FFN(x))

### ğŸ” 6. Stack of N Layers
A Transformer encoder typically stacks 6 to 12 of these layers.
Each layer builds more complex features â€” from shallow to deep understanding.


The final output of the encoder is:
- A sequence of vectors: one per token

- Each vector captures:
The word meaning
Its position
Its context and relation with all other words

- This output is then passed to:
* The decoder (in translation tasks), OR
* The next prediction head (like in BERT/GPT)**


| Component            | Role                                   |
| -------------------- | -------------------------------------- |
| Word Embedding       | Turns tokens into vectors              |
| Positional Encoding  | Adds order information                 |
| Self-Attention       | Each word learns context from others   |
| Feed Forward Network | Projects and transforms representation |
| Residual + LayerNorm | Stabilizes and strengthens training    |
| Stack of N layers    | Learns deeper contextual features      |